function [ h0, h1, h2 ] = Gradient( D, X, h0, h1, h2 )
%GRADIENT Summary of this function goes here
%   Detailed explanation goes here

i = 0 + 1i;

y = zeros(1, length(X)); %convolution(X, h0, 0) .* convolution(convolution(X, h2, 1), h1, 0);
e = zeros(1, length(X));
error = zeros(1, length(X));
del_h0 = zeros(1, length(X));

for k = length(h0):length(X)
    
    mult = convolution(X(k - length(h0) + 1:k), h0, 0) .* convolution(convolution(X(k - length(h0) + 1:k), h2, 1), h1, 0);
    y(k) = mult(length(mult));
    e(k) = D(k) - y(k);
    
    error(k) = e(k) * conj(e(k));
    
    s = convolution(convolution(X(k - length(h0) + 1:k),h2,1), h1, 0);

    learaning_rate_h0 = 0.00000001;
    for n = 1:length(h0)
        del_h0(n) = s(length(s)) * X(k - n + 1);
        
        real_h0 = e(k) * conj(del_h0(n)) + del_h0(n) * conj(e(k));
        imag_h0 = e(k) * conj(del_h0(n)) * i - i * del_h0(n) * conj(e(k));
        
        corr = real_h0 * learaning_rate_h0 + i*imag_h0 * learaning_rate_h0;
        h0(n) = h0(n) - corr;
        
    end
   
    
end

%
function Y = convolution(X, h, pow)

Y = zeros(1, length(X));

if(pow) == 1
    for i = 1:length(X)   
        for j = 1:length(h)
                Y(i) = Y(i) + h(j) * power(abs(X(i)), j-1);
        end
    end
   
else
    for i = length(h):length(X)   
        for j = 1:length(h)
                Y(i) = Y(i) + h(j) * X(i - j + 1);
        end
    end
end
        
end


X = [2+5i, 3-15i, 6+3i, 8+12i, 17+5i, 5-4i, 1+2i]
>> h0 = 0.1+0.2i, 0.2-0.4i,0.4+0.6i]
h2 = [0.2+0.4i, 0.3-0.1i, 0.6+0.2]
h1 = [0.1-0.3i, 0.2+0.8i, 0.5+0.9i]

#####################################################################################################

import numpy as np

def generate_signal(X):
    length_window = 0

    #additional fiter
    h0 = np.array([complex(0.1, 0.2), complex(0.2, -0.4), complex(0.4, 0.6)])
    X_add_h0 = np.array(np.zeros(len(X)), dtype = complex)
    for i in np.arange(len(h0) - 1, len(X)):
        for j in np.arange(len(h0)):
            X_add_h0[i] += h0[j] * X[i - j]


    h2 = np.array([complex(0.2, 0.4), complex(0.3, -0.1), complex(0.6, 0.2)])
    P = len(h2)
    X_pow = np.array(np.zeros(len(X)), dtype = complex)
    for i in np.arange(len(X)):
        for p in np.arange(P):
            X_pow[i] += h2[p] * pow(abs(X[i]), p)


    h1 = np.array([complex(0.1, -0.3), complex(0.2, 0.8), complex(0.5, 0.9)])
    X_pow_add_h1 = np.array(np.zeros(len(X)), dtype = complex)
    for i in np.arange(len(h0) - 1, len(X)):
        for j in np.arange(len(h1)):
            X_pow_add_h1[i] += h1[j] * X_pow[i - j]


    Y = X_add_h0 * X_pow_add_h1
    print(X, '\n\n', X_add_h0, '\n\n', X_pow_add_h1, '\n\n', Y, '\n\n')

    return Y

# alghoritm

def y_predict(X, length_window, P, h0, h1, h2):

    Y_pred = np.array(np.zeros(len(X)), dtype=complex)

    # additional fiter
    #h0 = complex(0, 0)
    X_add_h0 = np.array(np.zeros(len(X)), dtype=complex)
    for i in np.arange(length_window - 1, len(X)):
        for j in np.arange(length_window):
            X_add_h0[i] += h0[j] * X[i - j]

    #h2 = complex(0, 0)
    #P = 4
    X_pow = np.array(np.zeros(len(X)), dtype=complex)
    for i in np.arange(len(X)):
        for p in np.arange(P):
            X_pow[i] += h2[p] * pow(abs(X[i]), p)

    #h1 =  complex(0, 0)
    X_pow_add_h1 = np.array(np.zeros(len(X)), dtype=complex)
    for i in np.arange(length_window - 1, len(X)):
        for j in np.arange(length_window):
            X_pow_add_h1[i] += h1[j] * (X_pow[i - j])

    Y_pred = X_add_h0 * X_pow_add_h1

    return Y_pred

def h0_derivative(X, length_window, P, h1, h2):

    Y_pred = np.array(np.zeros(len(X)), dtype=complex)

    #h2 = complex(0, 0)
    #P = 4
    X_pow = np.array(np.zeros(len(X)), dtype=complex)
    for i in np.arange(len(X)):
        for p in np.arange(P):
            X_pow[i] += h2[p] * pow(abs(X[i]), p)

    #h1 =  complex(0, 0)
    X_pow_add_h1 = complex(0, 0)
    #for i in np.arange(len(X) - 1, -1, -1):
    i = len(X) - 1
    for j in np.arange(length_window):
        X_pow_add_h1 += h1[j] * (X_pow[i - j])

    Y_pred = X_pow_add_h1

    return -Y_pred

def h2_derivative(X, length_window, P, h0, h1):
    Y_pred = np.array(np.zeros(len(X)), dtype=complex)

    # additional fiter
    X_add_h0 = complex(0, 0)
    i = len(X) - 1
    for j in np.arange(length_window):
        X_add_h0 += h0[j] * X[i - j]

    # h2 = complex(0, 0)
    # P = 4
    X_pow = np.array(np.zeros(len(X)), dtype=complex)
    for i in np.arange(len(X)):
        for p in np.arange(P):
            X_pow[i] += pow(abs(X[i]), p)

    # h1 =  complex(0, 0)
    X_pow_add_h1 = complex(0, 0)
    i = len(X) - 1
    for j in np.arange(length_window):
        X_pow_add_h1 += h1[j] * (X_pow[i - j])

    Y_pred = X_add_h0 * X_pow_add_h1

    return -Y_pred

def h1_derivative(X, length_window, P, h0, h2):

    Y_pred = np.array(np.zeros(len(X)), dtype=complex)

    # additional fiter
    #h0 = complex(0, 0)
    X_add_h0 = complex(0, 0)
    i = len(X) - 1
    for j in np.arange(length_window):
        X_add_h0 += h0[j] * X[i - j]

    #h2 = complex(0, 0)
    #P = 4
    X_pow = np.array(np.zeros(len(X)), dtype=complex)
    for i in np.arange(len(X)):
        for p in np.arange(P):
            X_pow += h2[p] * pow(abs(X[i]), p)

    """"""
    Y_pred = X_add_h0 * X_pow

    return Y_pred

# error squared
def Error(y, X, N, P, h0, h1, h2):

    error = np.sum(y - y_predict(X, N, P, h0, h1, h2))

    return error#* complex.conjugate(error)


def Gradient(y, X, N, P, h0, h1, h2, nu = .001, mu = .000001, tetta = .00001, printing = True):

    for i in np.arange(10000):

        Errors = []

        for i in np.arange(len(X)):

            Errors.append(Error(y[i:i+N], X[i:i+N], N, P, h0, h1, h2))
            print(Errors)

            h0_re = np.array(h0.real)
            h0_im = np.array(h0.imag)

            for n in np.arange(len(h0)):
                error = Error(y[i:i+N], X[i:i+N], N, P, h0, h1, h2)
                error_= complex.conjugate(Error(y[i:i+N], X[i:i+N], N, P, h0, h1, h2))
                grad_ = complex.conjugate(h0_derivative(X[i:i+N], N, P, h1, h2))
                grad =  h0_derivative(X[i:i+N], N, P, h1, h2)


                im_unit = complex(0, 1)
                Re =  (error * -grad_ + -grad * error_).real
                Im = (error * -im_unit * grad_ + im_unit * grad * error_).real

                h0[n] = complex(h0[n].real - nu * (Re), h0[n].imag - nu * (Im))
                #h0[n].imag -= learning_rate * (Im)

            for n in np.arange(len(h2)):
                error = Error(y[i:i + N], X[i:i + N], N, P, h0, h1, h2)
                error_ = complex.conjugate(Error(y[i:i + N], X[i:i + N], N, P, h0, h1, h2))
                grad_ = complex.conjugate(h2_derivative(X[i:i + N], N, P, h0, h1))
                grad = h2_derivative(X[i:i + N], N, P, h0, h1)

                im_unit = complex(0, 1)
                Re = (error * -grad_ + -grad * error_).real
                Im = (error * -im_unit * grad_ + im_unit * grad * error_).real

                h2[n] = complex(h2[n].real - mu * (Re), h2[n].imag - mu * (Im))
                # h0[n].imag -= learning_rate * (Im)

            for n in np.arange(len(h1)):
                error = Error(y[i:i + N], X[i:i + N], N, P, h0, h1, h2)
                error_ = complex.conjugate(Error(y[i:i + N], X[i:i + N], N, P, h0, h1, h2))
                grad_ = complex.conjugate(h1_derivative(X[i:i + N], N, P, h0, h2))
                grad = h1_derivative(X[i:i + N], N, P, h0, h2)

                im_unit = complex(0, 1)
                Re = (error * -grad_ + -grad * error_).real
                Im = (error * -im_unit * grad_ + im_unit * grad * error_).real

                h1[n] = complex(h1[n].real - tetta * (Re), h1[n].imag - mu * (Im))
                # h0[n].imag -= learning_rate * (Im)


                if (printing):
                    print("Iteration {0}: MSE = {1:.6f}, h0 = {2:.6f}, h1 = {3:.6f}, h2 = {4:.6f}".format(i, Errors[i], h0, h1, h2))


#print(X.shape, d.shape)


X = np.array([complex(2, 5), complex(3, -15), complex(6, 3), complex(8, 12), complex(17, 5), complex(5, -4), complex(1, 2)])
Y = generate_signal(X)
h0 = np.array([complex(0.1, 0.2), complex(0.2, 0.1), complex(0.1, 0.2)])
h1 = np.array([complex(0.1, 0.1), complex(0.1, 0.1), complex(0.1, 0.1)])
h2 = np.array([complex(0.01, 0.02), complex(0.01, 0.02), complex(0.01, 0.02)])


Y_pred  = y_predict(X, 3, 3, h0, h1, h2)

print('\n\n',  X, '\n\n', Y, '\n\n', Y_pred)

Gradient(Y, X, 3, 3, h0, h1, h2, nu = .00001, mu = .0000001, tetta = .0000001, printing = True)



##################################################################################################

import numpy as np

def convolution(X, h, p = False):

    Y = np.array(np.zeros(len(X)), dtype=complex)

    if p == True:
        for i in np.arange(len(X)):
            for j in np.arange(len(h)):
                Y[i] += h[j] * np.power(abs(X[i]), j)

    else:
        for i in np.arange(len(h) - 1, len(X)):
            for j in np.arange(len(h)):
                Y[i] += h[j] * X[i - j]


    return Y



def Gradient(D, X, h0, h1, h2):

    i = complex(0, 1)

    y = np.array(np.zeros(len(X)), dtype=complex)
    e = np.array(np.zeros(len(X)), dtype=complex)
    error_ = np.array(np.zeros(len(X)), dtype=complex)
    delta_h0 = np.array(np.zeros(len(X)), dtype=complex)


    for k in np.arange(len(h0), len(X) - 1):
        mult = convolution(X[k - len(h0):k], h0, False) * convolution(convolution(X[k - len(h2):k], h2, True), h1, False)
        y[k - 1] = mult[len(h0) - 1]

        e[k - 1] = D[k - 1] - y[k - 1]
        error_[k - 1] = e[k - 1] * np.conj(e[k - 1])

        s = convolution(convolution(X[k - len(h2):k], h2, True), h1, False)

        learaning_rate_h0 = 0.0001
        for n in np.arange(len(h0)):
            #delta = s * learaning_rate_h0 * X[k - n - 1]
            #delta = delta[len(delta) - 1]

            #real_h0 = e[k - 1] * np.conj(delta) + delta * np.conj(e[k - 1])
            #imag_h0 = e[k - 1] * i * np.conj(delta) - i * delta * np.conj(e[k - 1])

            #corr = real_h0 * learaning_rate_h0 + i * imag_h0 * learaning_rate_h0

            delta = s * learaning_rate_h0 * X[k - n - 1]

            h0[n] = h0[n] - delta[len(delta) - 1]

            print(1)

        print(1)


    print (1)


h0 = np.array([complex(0.1, 0.2), complex(0.2, -0.4), complex(0.4, 0.6)])
h2 = np.array([complex(0.2, 0.4), complex(0.3, -0.1), complex(0.6, 0.2)])
h1 = np.array([complex(0.1, -0.3), complex(0.2, 0.8), complex(0.5, 0.9)])
X = np.array([complex(2, 5), complex(3, -15), complex(6, 3), complex(8, 12), complex(17, 5), complex(5, -4), complex(1, 2), complex(3, 5), complex(1, 3)])

X_h0 = convolution(X, h0, False)
X_h2 = convolution(X, h2, True)
X_h1 = convolution(X_h2, h1, False)

Y = X_h0 * X_h1


h0_ = np.array([complex(0.11, 0.19), complex(0.18, -0.36), complex(0.36, 0.59)])
Gradient(Y, X, h0_, h1, h2)

print(1)

##########################################################################################

function [ h0, h1, h2 ] = Gradient( D, X, h0, h1, h2 )
%GRADIENT Summary of this function goes here
%   Detailed explanation goes here

i = 0 + 1i;

y = zeros(1, length(X)); %convolution(X, h0, 0) .* convolution(convolution(X, h2, 1), h1, 0);
e = zeros(1, length(X));
error = zeros(1, length(X));
J_h0 = zeros(1, length(X));
J_h1 = zeros(1, length(X));
J_h2 = zeros(1, length(X));

for k = length(h0):length(X)
    
    mult = convolution(X(k - length(h0) + 1:k), h0, 0) .* convolution(convolution(X(k - length(h0) + 1:k), h2, 1), h1, 0);
    y(k) = mult(length(mult)); % current y
    
    e(k) = D(k) - y(k); %error in complex
    error(k) = e(k) * conj(e(k)); %error
    
    
    %h0---------------------------------------------------------------------
    s = conj(convolution(convolution(X(k - length(h0) + 1:k),h2,1), h1, 0)); %output of H1-block. CONJUG
    s = s(length(s));
    learaning_rate_h0 = 0.0000000000000000000001;
    
    for n = 1:length(h0)
        J_h0(n) = e(k) * s * X(k - n + 1);
        
        %real_h0 = e(k) * -conj(J_h0(n)) + -J_h0(n) * conj(e(k));
        %imag_h0 = e(k) * - i * conj(J_h0(n)) + i * J_h0(n) * conj(e(k));
        
        %corr = real_h0 * learaning_rate_h0 + i*imag_h0 * learaning_rate_h0;
        
        corr = conj(J_h0(n)) * learaning_rate_h0;
        h0(n) = h0(n) - corr;
        
    end
  
    %h1---------------------------------------------------------------------
    learaning_rate_h1 = 0.0000001;
    s = conj(convolution(X(k - length(h0) + 1:k), h0, 0)); %output of H0-block. CONJUG
    s = s(length(s));
    
    for n = 1:length(h1)
        J_h1(n) = e(k) * s * h2(n) * power(abs(X(k - length(h0) + 1)), n - 1);
        
        corr = conj(J_h1(n)) * learaning_rate_h1;
        h1(n) = h1(n) + corr;


    %h2------------------------------------------------------------------------------
    
    learaning_rate_h2 = 0.00000001;
    s = conj(convolution(X(k - length(h0) + 1:k), h0, 0)); %output of H0-block. CONJUG
    s = s(length(s)); %output of H0-block. CONJUG
    p = convolution(convolution(X(k - length(h0) + 1:k), ones(1, length(h2)), 1), h1, 0);
    p = p(length(p));
    for n = 1:length(h2)
        J_h2(n) = e(k) * s * p * power(abs(X(k - length(h0) + 1)), n - 1);
        
        
        corr = conj(J_h2(n)) * learaning_rate_h2;
        h2(n) = h2(n) + corr;
 
        
    end

end

end

############################################################################################################

function [ h0, h1, h2 ] = Gradient_Complex(  D, X, h0, h1, h2 )
%GRADIENT_COMPLEX Summary of this function goes here
%   Detailed explanation goes here

i = 0 + 1i;

y = zeros(1, length(X)); %convolution(X, h0, 0) .* convolution(convolution(X, h2, 1), h1, 0);
e = zeros(1, length(X));
error = zeros(1, length(X));
J_h0 = zeros(1, length(X));
J_h1 = zeros(1, length(X));
J_h2 = zeros(1, length(X));


for k = length(h0):length(X)
    
    %mult = convolution(X(k - length(h0) + 1:k), h0, 0) .* convolution(convolution(X(k - length(h0) + 1:k), h2, 1), h1, 0);
    
    y_current = Generate(X(k - length(h0) + 1:k),h0, h1, h2);
    y(k) = y_current(length(y_current)); % current y
    
    e(k) = D(k) - y(k); %error in complex
    error(k) = e(k) * conj(e(k)); %error
    
    
    %h0---------------------------------------------------------------------
    outB1 = convolution(convolution(X(k - length(h0) + 1:k),h2,1), h1, 0); %output of H1-block. CONJUG
    outB1 = outB1(length(outB1));
    learning_rate_h0 = 0.000001;
    
    for n = 1:length(h0)
        %$J_h0(n) = e(k) * s * X(k - n + 1);
        
        J_h0(n) =  e(k) * conj(outB1) * conj(X(k - n + 1));
        
        %real_h0 = e(k) * conj(J_h0(n)) + J_h0(n) * conj(e(k));
        %imag_h0 = e(k) * i * conj(J_h0(n)) + i * -J_h0(n) * conj(e(k));
        %corr = real_h0 * learaning_rate_h0 + i * imag_h0 * learaning_rate_h0;
        
        corr = learning_rate_h0 * J_h0(n);
        h0(n) = h0(n) - corr;
        
    end
    
     %h1---------------------------------------------------------------------
    
    %learning_rate_h1 = 0.000001;
    %outB0 = convolution(X(k - length(h0) + 1:k), h0, 0); %output of H0-block.
    %outB0 = outB0(length(outB0));
    
    %{for n = 1:length(h1)  
        
    %    f = convolution(X(k - length(h0) + 1:k), h2, 1);
   %     J_h1(n) =  e(k) * conj(outB0) * conj(f(length(h0) - n + 1));
        
  %      corr = learning_rate_h1 * J_h1(n);
 %       h1(n) = h1(n) - corr;

%end     
end



end






